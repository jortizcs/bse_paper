\section{Automating Metadata Acquisition}
%
%-building names dirty -> we need some way to characterize them
%-learn
%-input output example
%-who will generate these - maybe building manager, maybe users
%-ease of use as opposed to people writing regular expressoin themselves.
%-why not have someone write the regex expressions for those most common sensors ? because people who know are not the people who use the data. building managers know, people working closely with a sensor deployment know. Sometimes the regex might get complex ...... for instanct, when is a particular tag applicable ? 
%
%Why is it this way ?

In this section, we go into detail about how we apply program synthesis techniques and the input-output model of interaction to learn the various semantic labels contained inside a sensor's name\footnote{We use the words {\it sensor name} and {\it point name} interchangeably }, in effect boosting the metadata associated with a sensor.  We first introduce some basic terminology we will be using throughout this section, followed by an overview of the synthesis technique in Section~\ref{sec:synth}. We then describe how we adapt the technique to the context of sensor name qualification in Section~\ref{sec:adapt}.  We finally evaluate our adapted technique on our testbed in Section~\ref{sec:eval}.

%As explained earlier, most sensor names ( or SCADA\footnote{Supervisory Control and Data Acquisition} tags ) try to encode some level of semantic information. These names might have been generated by the individual who was in charge of commissioning a building, or by following a set of vendor-specific rules. In either case, the encoded sensor names convey very little meaning to people who are not familiar with the building layout.
%
%A lot of the sense points contained in a particular building is very site-specific, e.g a building may contain a fault detector for its backup chiller, for which there might not be a well-defined sensor labelling schema from the vendor. In such scenarios, the agent commissioning a building tries his best to convey the information by using indicative labels. E.g we encountered points labelled \texttt{BLDA4S18SASA\_M}, which denotes an alarm if the supply air temperature setpoint for a supply fan gets too high\footnote{The point is to be read thus : the building name is denoted by {\it BLD}, {\it A4} indicates an air handling unit whose identification number if $4$. {\it S$18$} denotes a supple fan whose identification number if $18$, and {\it SASA\_M} denotes a supply alarm setpoint}. 

\subsection{Terminology}

The expert is expected to point out \emph{(Tag Name, Tag Value, Value Type)} tuples in the sensor name. A {\it tag} is mapped on to a substring of the sensor name, which is called its {\it value}. A tag can have a constant or a variable value. A value should be regarded a \emph{constant} if it is not specific to that particular sensor, and \emph{variable} otherwise.

{\it Sample Input:} Suppose the expert is presented with an example \texttt{BLDA1R465\_\_ART}. Suppose this sensor name indicates that it is in Building \texttt{BLD}, is part of the first air handling unit, indicated by the character \texttt{A1}, in room $465$ (\texttt{R465}) and it is the area temperature sensor (\texttt{ART}). He should qualify it in order as

 \texttt{BLDA1R465\_\_ART} : (site, \texttt{BLD}, const), (ahu, \texttt{A}, const), (ahuRef,\texttt{1}, var), (zone,\texttt{R}, const), (zoneRef, \texttt{465}, var), (zone air temp sensor, \texttt{ART}, const). 

In this example the {\bf site} tag's value is \texttt{BLD}, which is not specific to that particular sensor. Hence, the expert should mark it as a constant. On the other hand, the value of the {\bf ZoneRef} tag is specific to that sensor, and hence should be marked as variable.

{\it Sample Output:} The synthesis technique should then be able to identify the tags in a new sensor name automatically. For example, given the sensor name \texttt{BLDA5R234\_\_\_ART}, it should output the set of tuples shown below:

\texttt{BLDA5R234\_\_\_ART} : (site, \texttt{BLD}), (ahu,\texttt{A}), (ahuRef,\texttt{5}), (zone,\texttt{R}), (zoneRef,\texttt{465}), (zone air temp sensor,\texttt{ART}).

We term each of these tuples as a {\it qualification}, because it qualifies a set of alphanumeric characters into normalized metadata tags. We term the output as a {\it full qualification}, if every alphanumeric character in the sensor name was {\it qualified} by the set of outputed {\it tags}.

{\it Tag Names :} The goal of the expert should be to use tag names from a normalized building equipment taxonomy schema that has been widely adopted. Currently, there is no consensus schema in the sensor network, or building management system vendor community about a particular schema. Some schemas such as Green Building XML~\cite{GBXML}, and Industry Foundation Classes~\cite{IFC} require a very high level of detail for every metadata tag, making it unsuitabe for use in our context where that detail might not be available. Instead, we assume that the expert qualifies the sensor names with metadata tags that have been developed as a part of the Project Haystack effor~\cite{haystack}, which is an open source effort to develop taxonomies and ontologies for building equipment. This schema is, however, very limited, and not applicable to a wide variety of building-specific sensor points (such as specific alarms, etc). In these cases, we expect the expert to use an easily understandable long-form tag, which is consistent across the entire building. This can be achieved by presenting the expert a set of tags that has already been used in a that building to qualify a particular substring. 


\subsection{Synthesis technique overview}
\label{sec:synth}

\begin{figure}[h!]
  
  \centering
    \includegraphics[width=0.5\textwidth]{figs/stringLanguage.pdf}
\caption{Language for learning substring extraction}
\label{fig:language}
\end{figure}


We will first describe the high-level logic of the synthesis technique in ~\cite{Gulwani:2011} which synthesizes simple regular expressions to transform input columns of a desired spreadsheet to a desired output column. The details of the technique is available in the corresponding paper. We will then describe the set of specific changes to adapt this technique to our context. 

{\bf The Algorithm:}

The main aim of the technique is to learn two sets of information from the inputed examples --- (a) whether a string transformation is applicable on a particular input, and (b) what is the set of regular expressions that transform the input string to the output string.

From each user-provided example, the set of all expressions from the language (shown in Figure~\ref{fig:language}), that could extract the required substring from that input is computed. If there are multiple input-output examples, the substring extraction rules of the multiple examples are intersected to obtain a more concise set of expressions. If the extraction expression sets cannot be intersected, they are maintained as two disjoint sets, which we shall hereby term as a {\it partitions}. 

Finally, for each disjoint set of extraction expressions, a boolean classifier is built in the DNF form, to differentiate the examples in one partition to examples in all other partitions. When a new string is given to this tool, the classifier is applied on it to figure out which partition the new input falls into, and the corresponding set of transformation expressions are then applied on it.

{\bf The Language:}

The top level expression of the language is the classifier --- the {\bf Switch}$(b_i,e_i)$ function, which applies the substring expression $e_i$ to the input only if it matches the boolean expression $b_i$. The boolean function is in DNF form and is composed of predicates of the form {\bf Match}$(v_i,r,k)$ , which evaluates to true, iff the input $v_i$ has $k$ occurences of the regular expression $r$.

The Substring expression {\bf SubStr}$(v_i,p_1,p_2)$, evaluates to the substring between positions $p_1$ and $p_2$ of the string $v_i$. {\bf CPos}$(k)$ denotes the position $k$ in the substring. A position expression {\bf pos}$(r_1,r_2,c)$ when applied on a string $s$ evaluates to a position $t$ in the subject string $s$ such that $r_1$ matches some suffix $s[0 ..t]$ and $r_2$ matches some prefix of $s[t ... l]$ (where $l$ = Length$(s)$). Also, $t$ is the $c$th such match starting from the left end of the string. The regular expressions are either just a single token $\tau$, or a token sequence, {\bf TokenSequence}$(\tau_1 .. \tau_n)$, or $\epsilon$ (which matches the empty string). The tokens $\tau$ range over some token classes, e.g one token for alphabetic characters, one for numeric characters, and one for each special character. 

We provide a couple of examples to elucidate how this technique works. 

{\bf Example 1}:

Consider, again, the sensor name \texttt{BLDA1R465\_\_ART}. The substring \texttt{ART} can be obtained, among other expressions, by either of the following language transformations :  SubString$(s, $ Cpos$(11)$, Cpos$(14))$, or SubString$(s,$ Pos(UnderscoreToken, AlphToken,$1$), Pos(AlphToken,$\epsilon, 1))$.

{\bf Example 2}: 

Suppose the synthesis algorithm has seen two examples \texttt{BLDA1R465\_\_ART}, whose desired output is \texttt{A} at index 3,  and \texttt{BLD\_\_R479\_ART}, whose desired output is \texttt{479}. One of the possible expressions that the synthesis algorithm can come up with is : Switch($(b_1, e_1), (b_2,e_2)$, where $b_1 =$ Match($s$, AlphToken UnderscoreToken,$1$), $e_1=$ Substring($s$, Cpos(3), Pos(AlphToken, NumToken,1)), and $b_2=$ Match($s$, UnderscoreToken AlphToken, 2) and $e_2$ = SubString ($s$, Cpos(6), Cpos(9)).

In general, there can be many expressions in the defined language that can obtain the desired substring, and provide a classifier to specify which types of inputs each type of substring extraction should work on.

%
%{\bf Example:} 
%
%\begin{list}
%\item example 1
%\item example 2
%
%\end{list}

\subsection{Adapting to our context}
\label{sec:adapt}

The intuition behind adapting this synthesis technique to our context is that we can independently consider each ({\bf tag}) to be a potential output for that string. If the tag can be applied to qualify a substring of the sensor name, then the output would be the required substring. In all other cases, the output would be $\epsilon$ or the null  string. 

Thus, for each given example and for each tag in that example, we can compute the set of all expressions from the language that could extract the required substring from that sensor name. We then learn a boolean classifier such that all tags that are present in the example evaluate their {\bf Match} condition on this example to True, and all the tags that are not, evaluate theirs to be False.

If the same tag is present in multiple examples, the tag's new extraction expression is the intersection of it's extraction expression sets for each of those examples. This may result in disjoint {\it partitions} for each tag. Finally, a boolean classifier is built to differentiate examples of each partition from all other provided examples. 

However, we run into various challenges if we directly apply this technique to the sensor naming. We conducted an experiment where we provided examples for sensor names in a building containing 1585 sense points, applying the technique with no adaptation. Sensor names were chosen at random from the corups of all sensor names, and a full qualification of it was provided as the next input. Figure~\ref{fig:simpleTokenNoCoverage} shows the result of our experiment.

We would expect the percentage of sensor names to increase with each added example. We find this trend up to around 25 examples, after which the synthesis technique started adding qualifying substrings of the sensor name to erroneous tags. At closer inspection, it turns out that the tokens  (one for alphabetic characters, one for numeric characters, and one for each special character) used by the regular expressions and the {\bf Match} predicates were not expressive enough to capture the difference of applicability of different tags. 

To illustrate the problem, consider the examples 

{\it Example 1} : \texttt{BLDA4S1831\_STA} : [ (site, \texttt{BLD}, const), (ahu, \texttt{A}, const), (ahuRef,\texttt{4}, var), (supply fan,\texttt{S}, const), (supply fanRef,\texttt{1831}, var), (status point,\texttt{STA},const) ] ; and 

{\it Example 2}: \texttt{BLDA3R5871\_VAV} : [ (site, \texttt{BLD}, const), (ahu, \texttt{A}, const), (ahuRef, \texttt{3}, var), (zone, \texttt{R}, const), (zoneRef, \texttt{5871}, var), (vav, \texttt{VAV}, const) ]

Both these sensor names have the exact same arrangement of numeric and alphabetic characters, and special symbols, and no regular expression used by {\bf Match}$(v_i,r,c)$  would be able to discern between the two. The result would either be both tags being mapped to the same substring in an effort to qualify it, or neither. Hence, we modify the existing language and set of tokens to stay general enough, yet be more expressive.

\begin{figure}[h!]
  
  \centering
    \includegraphics[width=0.5\textwidth]{figs/gulwani-noconverge.pdf}
\caption{Number of sensor names fully qualified with a token set consisting simply of alphabetic characters [A-Z], numerals [0-9], and one token for each special character.}
\label{fig:simpleTokenNoCoverage}
\end{figure}

\subsubsection{Challenges}

Adaptation of the synthesis technique to our context involves certain challenges. First, the number of tags required to fully qualify an entire building might be large, wheras certain tags may be applicable on a very limited number of sensor names. In such a case, the {\bf Match}$(v_i,r,c)$ and regular expressions learnt for each tag should be expressive enough to differentiate a small group of sensor names from the remaining. 

Second, the tokens used in the regular expressions need to be expressive enough to capture the correct substring expressions. Different conventions of sensor naming from building to building precludes us from having an apriori set of special tokens to increase the effectiveness of our regular expressions.

Third, a building may have 1000s of sensor points, making visual inspection of correctness of sensor name qualification very hard. Hence, wrong application of a tag to a particular sensor name is likely to go unnoticed. We mitigate this by being more conservative with the boolean preciates in the top level {\bf Switch} operator. 

% incremental intersection . 

Below, we list four techniques that augment the basic language shown in Figure~\ref{fig:language} to enable correct classification of 

\subsubsection{Token Set}

In the example we showed earlier, the set of regular expressions were not powerful enough to differentiate between two sensor names if we use one token for alphabetic characters, one for numeric characters, and one for each special symbol. An alternative would be to use a different token for every alphabet, which would be able to discern between the two sensor names in the previous example. However, this makes the set of regular expressions obtained very specific to this particular building. One building in our testbed had variable values which consisted of a combination of alphabets and numerals , e.g \texttt{BLD2.PWR.CL42A.REACTIVE POWER}, which describes that is a sensor point in building \texttt{BLD2}, that senses the reactive power of the power meter \texttt{CL42A}, which is a variable value (since it is sensor-specific). This building also had other power meters which had identifications of the form \texttt{DM12}, etc. Using a token for each different alphabetic character would not be able to express both of them as outputs to the {\bf power meter Ref} tag without multiple examples.

Hence, we utilize the constant tag values in the examples provided by the expert as different tokens, in addition to the normal tokens described above. Thus, a sensor name \texttt{BLDA1R465\_\_ART} is treated as a set of tokens [ (BLD)(A)[0-9](R)[0-9]+ UnderscoreTok UnderscoreTok (ART) ], and the sensor name \texttt{BLD2.PWR.CL42A.REACTIVE POWER} is treated as a combination of tokens [ (BLD2) dotTok (PWR) dotTok [A-Z]+[0-9]+[A-Z] dotTok (REACTIVE POWER). All the regular expressions generated for a sensor name utilize this token structure. Note that this is able to differentiate between both the sensor points in the example above. 

\subsubsection{Splitting constant tags}

In order to improve the efficiency of the classifier, we consider a tag having multiple constant values, as different tags. For instance, in our test building, an {\it exhaust fan} tag was applicable either as the constant character \texttt{E} or the constant character \texttt{EF}. Note that the index of the various other tag's values in a sensor name change depending on which of the constant value appears, rendering the Cpos(k) operator useless. 

We treat the same tags which maybe represented by two different constant string to be two different tags altogether. This enables a richer intersection set when tag outputs from the expert-given examples are combined. 

\subsubsection{Modifying Boolean Classifiers}

Since manual inspection of the qualification of all the points is not feasible, we take steps to strengthen the boolean classifiers corresponding to each tag. We implement different strategies for tags that have constant and variable values.

If a tag has a constant value, we ensure that each clause in the DNF expression used by the {\bf Switch} operator has a {\bf Match}($s$, constTagValue, $k$) as the first boolean operator, where $k$ is the number of times the constant value appears in the example. This ensures that if a tag is evaluated to be applicable on a string, the constant substring does exist in the string.

Next, in cases where a tag which has a variable value, and is a reference to another tag with a constant value which is applicable on the same string, we mandate that the constant tag also have been deemed applicable on the same sensor name. For instance, in the example \texttt{BLDA1465\_ART}, the tag {\bf zoneRef} whose value is to \texttt{465} has a reference to the {\bf zone} tag, which has a constant value. Thus, we would only evaluate the {\bf Match} expressions for {\bf zoneRef} iff the the {\bf Match} condition for the {\bf zone} tag has also evaluated to true. 




\subsubsection{Modifying Matching expression}

To make the classifiers more general, so that regular expressions learnt for one building may be applicable to sensor names of other buildings with a similar naming convention, we augment the language to include Match arguments where it evaluates to true iff the regular expression is satisfied at position equal to $k$. 

Suppose, from the technique encounters only one example \texttt{BLDA1R465\_\_ART} , and for the {\bf zone temp sensor} tag learns a classifier  {\bf Match}$(s,$(ART),1) for application of the {\bf zone temp sensor} tag. Now, if another building was abbreviated as \texttt{ART}, and the synthesis technique encountered a sensor name of the form \texttt{ARTA2R354\_\_ART}, the classifier would fail because there are two matches to the regular expression (ART) in the string. 

However, utilizing the position of a regular expression with a higher priority while generating classifiers mitigates some of the issues which arise from using the previously defined {\bf Match} token.



\section{Evaluation of Learning By Example}
\label{sec:eval}

In this section, we guage the effectiveness of our learning by example technique by evaluating the number of examples required to qualify labels in two large commercial buildings. 

\subsection{Testbed}

We manually generated ground truth data for all the points in two buildings whose building management system was installed by different vendors. Building $1$ has 1586 sensor points and was built in the 1990s. Building $2$ was built in the 2000s and has 2551 sense points. The label characteristics of the two buildings are shown in Figure~\ref{fig:buildingLabelCharacteristics}. 

Figure~\ref{fig:labelFreq} shows that in  these two buildings, a few tags (about 20 in each building) frequently appear in a lot of sensor names. This is pretty common in commercial buildings, where a majority of the points are related to zone or room information. For instance, Building $1$, has a room setpoint sensor, an airflow sensor and temperature sensor for each of its more than 200 rooms. Each of these points have a label indicating that they are a room, and a room number. These most frequent tags also fully qualify a large number of the sensor names in both buildings. In other words, learning proper classifiers and qualifications for about 20 labels could yield a full qualification for 70-80\% of the sensor names in both these buildings.

The distribution frequency of labels also has a long tail. For both the buildings, the labels corresponding to {\it site} and {\it zoneRef} are most common. However, the distribution of labels show that there is also a long tail. These comprise of building specific sensors, alarms or status variables. There are also a lot of incomplete or inconsistent labeling of sensor names, the tags of which fall in this long tail. Thus, one of the main objectives of the learning algorithm is that it does not learn wrong classifiers for tags based on sensor names that fall in this long tail.

\begin{figure}[h!]]
\centering
	\begin{subfigure}{0.48\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figs/pointOccuranceFreq.pdf}
                \caption{Percentage of sensor names each tag appears in. The x-axis is sorted according to the frequency of occurence of a label}
                \label{fig:labelFreq}
	\end{subfigure}
	\begin{subfigure}{0.48\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figs/pointCDF.pdf}
                \caption{Percentage of sensor names fully qualified by the highest ranking tags. A point ($x$,$y$) indicates that $y$ sensor names could be fully qualified by using labels ranked $1$ ... $x$}
                \label{fig:pointCDF}
	\end{subfigure}
\caption{Characteristics of tag application from two buildings we generated ground-truth data for, to test our learning technique}
\label{fig:buildingLabelCharacteristics}
\end{figure}


\subsection{Choosing the Next Example}


The large number of sensors in a building pose a challenge in selecting the next example to present to the expert. First, the expert might not always be able to browse through all sensor points to check correct qualification. Also, an expert might visally not be able to discern which points would add the most amount of information to the learning process. 

We implemented four different generators to evaluate which example should be provided next to the expert:

{\bf Random:} This generator just finds at random the next example to present to the expert. While choosing the example, the random algorithm chooses among the set of sensor names which it feels it has not been able to fully qualify.  

{\bf MinRemaining :} This generator chooses the example, that according to our tool, has the minimum string length left to qualify. The intuition behind this is to gain more concrete knowledge about a small number of labels.

{\bf MaxRemaining :} This chooses the example, that the learning technique feels has the maximum string length left to qualify. These examples would help the learning technique gain coverage over the space of unsees labels. The more labels the learning technique knows, the more sensor name information it will be able to qualify.

{\bf Self Correcting :} There are some sensor names where the learning algorithm can itself figure out that it has incorrectly qualified a sensor name. There can be three such indicators. First, for a sensor name which has matched its boolean classifier, but none of its set of {\bf SubString} regular expressions is applicable. Second, if the sensor name has been qualified with labels that overlap over the same substring. Third, the learning algorithm can get a notion of the qualification uncertainty of a sensor name, if its (tag, value) tuples change drastically when a new example has been added. This generator gives the expert the examples that satisfy the most number of these three criteria. Once, none of the points satisfy these criteria, this generator defaults to the MinRemaining generator.


We wrote a script that automatically gave the learning algorithm the example that it asked for, and evaluated the qualifications provided by the algorithm. We terminated when the number of correct full sensor qualifications reached 90\%. A full qualification of a sensor name into (tag, value) tuples is correct (a) the correct tags were applied on it and  obtained the correct values corresponding to each tag, (b) No extra incorrect tag was applied to the sensor name, and (c) the tags were able to fully qualify every alphanumeric character of the sensor name.

Figures ~\ref{fig:active-learning} show the results of the four generators on the two buildings. The \emph{Random} generator took the least number of examples to achieve full qualification of 70\% of the sensor names, achieving it much quicker than the others. The reason for this is due to the long tail of the label distribution of tag names (  Figure~\ref{ig:buildingLabelCharacteristics}). The top 20 most occurring tags, by themselves, can fully qualify about the majority of the sensor names. A random generator has a high probability of finding one of these points, thus acquanting itself more quickly of the most frequently occuring labels. Neither of the other three classifiers is able to achieve that. They get stuck trying to learn regular expressions from sensors with obscure tags (\emph{MinRemaining}), or trying to cover more labels by first qualifying sensors which have been least qualified, which comprise mainly of sensor names which are inconsistently named (\emph{MaxRemaining}), or by choosing form the set of ill-formed sensor names, which would indicate errors to the learning algorithm (\emph{Self-Correcting}).


\begin{figure}[h!]
\centering
	\begin{subfigure}{0.48\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figs/soda-active-learning.pdf}
                \caption{Building 1}
                \label{fig:active-learning-soda}
	\end{subfigure}
	\begin{subfigure}{0.48\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figs/sdh-active-learning.pdf}
                \caption{Building 2}
                \label{fig:active-learning-sdh}
	\end{subfigure}
\caption{The number of examples required to fully qualify 70\% of sensor names in two buildings}
\label{fig:active-learning}
\end{figure}



\subsection{Applying to other buildings}


\begin{figure}[h!]
  
  \centering
    \includegraphics[width=0.5\textwidth]{figs/campusWideStats.pdf}
\caption{Applicability of the tags learnt from Building $1$ to other buildings in our testbed.A tag is applicable to another point name if it either is the same constant value as encountered in Building $1$, or if the point name has a variable substring which could be qualified using the same tags as found in Building $1$.  The x-axis are the tags ordered by their frequency of occurence in Building $1$. If there is no value corresponding to a particular value on the x-axis, it indicates that the particular tag was not applicable to any other building. The primary y-axis shows the percentage of total point names in the remaining buildings the same tags are applicable to. The secondary y-axis shows the percentage of the number of buildings a particular tag may be used to qualify a sensor point of.  }
\label{fig:campusWideStats}
\end{figure}




One of the major goals of learning from examples provided by an expert is that the learnt regular expressions may be applicable to other buildings which have similar naming conventions. In this section, we evaluate the efficacy of the regular expressions learnt on Building $1$ of our testbed with the remaining 55 buildings on campus, each of which was commissioned by the same vendor. As mentioned before, the remaining buildings comprise about 16,000 sensor points.

Figure~\ref{fig:campusWideStats} shows the applicability of tags learnt from Building $1$ to the remaining buildings. Tags which were most frequent in Building $1$ such as {\bf zone, zoneRef} and {\bf ahuRef}, were also widely applicable throughout other buildings. This is expected because all these buildings are commercial offices, with a large number of zone, and a set of points associated with each zone. There are also a few tags which are infrequent in a particular building, but applicable across all buildings, e.g the {\bf return water temp} tag or the {\bf outside air temp} tag. Thus, fully qualifying even one building has the potential to yield regular expressions that can then be propagated across buildings.

It was impractical to ground truth all the buildings and its 16,000 sensor points for our experiment. Hence, we hand wrote regular expressions to the best of our efforts to qualify sensor names in those buildings. It is this manual qualification data that we treat as ground truth in the our experiment. We run three experiments, where we use the regular expressions obtained after 50, 70 and 90\% full qualification results obtained by the {\bf Random} process in the previous experiment(see Figure~\ref{fig:active-learning-soda}).While running these expressions on a different building, we just replaced the token corresponding to the tag {\bf site} to the building-specific {\bf site}  token. 

Figure~\ref{fig:recallCampuswide} shows the true-positive rate based for each tag based on our ground truth data. The true positive rate indicates that the required tag was correctly applied where it was applicable, and it was able to qualify the correct substring from the corresponding sensor name. 

From the graphs we can see that there is direct effort-applicability correlation. The more fully qualified a particular building is, the more tag applicability expressions are learnt. When enough examples were provided such that Building $1$ had correctly fully qualified only 50\% of the sensor names, the synthesis technique had either not encountered a lot of the lesser frequent tags, or did not have enough examples to build a robust classifier, leaving the regular expression set unable to discover the same tags in other buildings. However, when enough examples for 90\% full qualification was provided, the synthesis algorithm was able to correctly identify and qualify sensor names in other buildings with the correct tag and value. 

Some of the tags could not be applicable to other buildings, because they use different constant values for the same tag name. For instance, the {\bf exhaust fan} tag is specified in certain buildings as \texttt{EF}, wheras in Building $1$ it was specified as \texttt{E}. Also, certain tags which extract variable values (the {\bf zoneRef} tag) face some challenges when scaling across buildings. 

\subsection{Discussion}



\begin{figure*}[ht!]
\centering
	\begin{subfigure}{0.4\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figs/recallCampusWide-50.pdf}
                \caption{Regular expressions obtained after 50\% full qualification by {\bf Random}}
		\label{fig:full50}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figs/recallCampusWide-70.pdf}
                \caption{Regular expressions obtained after 70\% full qualification by {\bf Random}}
		\label{fig:full70}
	\end{subfigure}
	\begin{subfigure}{0.4\textwidth}
                \centering
		\includegraphics[width=\textwidth]{./figs/recallCampusWide-90.pdf}
                \caption{Regular expressions obtained after 90\% full qualification by {\bf Random}}
		\label{fig:full90}
	\end{subfigure}
\caption{This Figure shows the efficacy of using the regular expressions learned on Building $1$ in qualifying point names with similar tags in other buildings, for three different set of expressions obtained from our language after applying the {\bf Random} example generator in the previous experiment until it had achieved 50, 70 and 90\% full qualifications of sensor names in Building $1$. The x-axis are the tags ordered by their frequency of occurence in Building $1$. The primary y-axis shows the true positive rate of identifying a particular tag in the point names in other buildings as compared to our ground truth data. The secondary y-axis shows the true positive rate rate of identifying a tag in a different building as compared to our ground truth data.}
\label{fig:recallCampuswide}
\end{figure*}

%
%\begin{figure}[h!]
%\centering
%	\begin{subfigure}{0.48\textwidth}
%                \centering
%		\includegraphics[width=\textwidth]{./figs/campusWideStats.pdf}
%                \caption{Building 1}
%                \label{fig:campusWideStats}
%	\end{subfigure}
%	\begin{subfigure}{0.48\textwidth}
%                \centering
%		\includegraphics[width=\textwidth]{./figs/recallCampusWide.pdf}
%                \caption{Building 2}
%                \label{fig:recallCampusWide}
%	\end{subfigure}
%\caption{The number of examples required to fully qualify 70\% of sensor names in two buildings}
%\label{fig:active-learning}
%\end{figure}









